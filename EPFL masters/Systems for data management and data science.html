<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Systems for data management and data science</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="css/pandoc.css" />
  <style>
    html {
      font-size: 100%;
      overflow-y: scroll;
      -webkit-text-size-adjust: 100%;
      -ms-text-size-adjust: 100%;
    }

    body {
      color: #444;
      font-family: Georgia, Palatino, "Palatino Linotype", Times,
        "Times New Roman", serif;
      font-size: 12px;
      line-height: 1.7;
      padding: 1em;
      margin: auto;
      max-width: 42em;
      background: #fefefe;
    }

    a {
      color: #0645ad;
      text-decoration: none;
    }

    a:visited {
      color: #0b0080;
    }

    a:hover {
      color: #06e;
    }

    a:active {
      color: #faa700;
    }

    a:focus {
      outline: thin dotted;
    }

    *::-moz-selection {
      background: rgba(255, 255, 0, 0.3);
      color: #000;
    }

    *::selection {
      background: rgba(255, 255, 0, 0.3);
      color: #000;
    }

    a::-moz-selection {
      background: rgba(255, 255, 0, 0.3);
      color: #0645ad;
    }

    a::selection {
      background: rgba(255, 255, 0, 0.3);
      color: #0645ad;
    }

    p {
      margin: 1em 0;
    }

    img {
      max-width: 100%;
    }

    h1,
    h2,
    h3,
    h4,
    h5,
    h6 {
      color: #111;
      line-height: 125%;
      margin-top: 2em;
      font-weight: normal;
    }

    h4,
    h5,
    h6 {
      font-weight: bold;
    }

    h1 {
      font-size: 2.5em;
    }

    h2 {
      font-size: 2em;
    }

    h3 {
      font-size: 1.5em;
    }

    h4 {
      font-size: 1.2em;
    }

    h5 {
      font-size: 1em;
    }

    h6 {
      font-size: 0.9em;
    }

    blockquote {
      color: #666666;
      margin: 0;
      padding-left: 3em;
      border-left: 0.5em #eee solid;
    }

    hr {
      display: block;
      height: 2px;
      border: 0;
      border-top: 1px solid #aaa;
      border-bottom: 1px solid #eee;
      margin: 1em 0;
      padding: 0;
    }

    pre,
    code,
    kbd,
    samp {
      color: #000;
      font-family: monospace, monospace;
      _font-family: "courier new", monospace;
      font-size: 0.98em;
    }

    pre {
      white-space: pre;
      white-space: pre-wrap;
      word-wrap: break-word;
    }

    b,
    strong {
      font-weight: bold;
    }

    dfn {
      font-style: italic;
    }

    ins {
      background: #ff9;
      color: #000;
      text-decoration: none;
    }

    mark {
      background: #ff0;
      color: #000;
      font-style: italic;
      font-weight: bold;
    }

    sub,
    sup {
      font-size: 75%;
      line-height: 0;
      position: relative;
      vertical-align: baseline;
    }

    sup {
      top: -0.5em;
    }

    sub {
      bottom: -0.25em;
    }

    ul,
    ol {
      margin: 1em 0;
      padding: 0 0 0 2em;
    }

    li p:last-child {
      margin-bottom: 0;
    }

    ul ul,
    ol ol {
      margin: 0.3em 0;
    }

    dl {
      margin-bottom: 1em;
    }

    dt {
      font-weight: bold;
      margin-bottom: 0.8em;
    }

    dd {
      margin: 0 0 0.8em 2em;
    }

    dd:last-child {
      margin-bottom: 0;
    }

    img {
      border: 0;
      -ms-interpolation-mode: bicubic;
      vertical-align: middle;
    }

    figure {
      display: block;
      text-align: center;
      margin: 1em 0;
    }

    figure img {
      border: none;
      margin: 0 auto;
    }

    figcaption {
      font-size: 0.8em;
      font-style: italic;
      margin: 0 0 0.8em;
    }

    table {
      margin-bottom: 2em;
      border-bottom: 1px solid #ddd;
      border-right: 1px solid #ddd;
      border-spacing: 0;
      border-collapse: collapse;
    }

    table th {
      padding: 0.2em 1em;
      background-color: #eee;
      border-top: 1px solid #ddd;
      border-left: 1px solid #ddd;
    }

    table td {
      padding: 0.2em 1em;
      border-top: 1px solid #ddd;
      border-left: 1px solid #ddd;
      vertical-align: top;
    }

    .author {
      font-size: 1.2em;
      text-align: center;
    }

    @media only screen and (min-width: 480px) {
      body {
        font-size: 14px;
      }
    }
    @media only screen and (min-width: 768px) {
      body {
        font-size: 16px;
      }
    }
    @media print {
      * {
        background: transparent !important;
        color: black !important;
        filter: none !important;
        -ms-filter: none !important;
      }

      body {
        font-size: 12pt;
        max-width: 100%;
      }

      a,
      a:visited {
        text-decoration: underline;
      }

      hr {
        height: 1px;
        border: 0;
        border-bottom: 1px solid black;
      }

      a[href]:after {
        content: " (" attr(href) ")";
      }

      abbr[title]:after {
        content: " (" attr(title) ")";
      }

      .ir a:after,
      a[href^="javascript:"]:after,
      a[href^="#"]:after {
        content: "";
      }

      pre,
      blockquote {
        border: 1px solid #999;
        padding-right: 1em;
        page-break-inside: avoid;
      }

      tr,
      img {
        page-break-inside: avoid;
      }

      img {
        max-width: 100% !important;
      }

      @page :left {
        margin: 15mm 20mm 15mm 10mm;
      }

      @page :right {
        margin: 15mm 10mm 15mm 20mm;
      }

      p,
      h2,
      h3 {
        orphans: 3;
        widows: 3;
      }

      h2,
      h3 {
        page-break-after: avoid;
      }
    }
  </style>
  <script src="https://cdn.jsdelivr.net/npm/darkreader@4.7.15/darkreader.min.js"></script>
  <script>
    DarkReader.enable({
      brightness: 100,
      contrast: 90,
      sepia: 10,
    });
  </script>
  <link
    rel="icon"
    type="image/png"
    href="https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcTJg97A-Sa8mxjkRCmjR51WjHATLvq2aF89Z1CprR2WcQ60qYZC"
  />
  <meta name="theme-color" content="#252525" />
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Systems for data management and data science</h1>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#storage" id="toc-storage">Storage</a>
<ul>
<li><a href="#storage-hierarchy" id="toc-storage-hierarchy">Storage
hierarchy</a></li>
<li><a href="#file-storage" id="toc-file-storage">File storage</a></li>
<li><a href="#page-layout-for-relational-data"
id="toc-page-layout-for-relational-data">Page layout (for relational
data)</a></li>
<li><a href="#partition-attributes-across-pax"
id="toc-partition-attributes-across-pax">Partition Attributes Across
(PAX)</a></li>
</ul></li>
<li><a href="#query-execution-and-optimization"
id="toc-query-execution-and-optimization">Query execution and
optimization</a>
<ul>
<li><a href="#query-execution" id="toc-query-execution">Query
execution</a></li>
<li><a href="#query-optimization" id="toc-query-optimization">Query
optimization</a></li>
</ul></li>
<li><a href="#execution-models-for-distributed-computing"
id="toc-execution-models-for-distributed-computing">Execution models for
distributed computing</a>
<ul>
<li><a href="#big-data" id="toc-big-data">Big data</a></li>
<li><a href="#query-models" id="toc-query-models">Query models</a></li>
<li><a href="#mapreduce" id="toc-mapreduce">MapReduce</a></li>
<li><a href="#spark" id="toc-spark">Spark</a></li>
</ul></li>
<li><a href="#concurrency-control"
id="toc-concurrency-control">Concurrency control</a>
<ul>
<li><a href="#acid-transaction-schedules"
id="toc-acid-transaction-schedules">ACID &amp; Transaction
Schedules</a></li>
<li><a href="#pessimistic-concurrency-control-protocols"
id="toc-pessimistic-concurrency-control-protocols">Pessimistic
concurrency control protocols</a></li>
<li><a href="#optimistic-concurrency-control-protocols"
id="toc-optimistic-concurrency-control-protocols">Optimistic concurrency
control protocols</a></li>
<li><a href="#timestamp-based-cc"
id="toc-timestamp-based-cc">Timestamp-based CC</a></li>
<li><a href="#multiversion-cc" id="toc-multiversion-cc">Multiversion
CC</a></li>
<li><a href="#bottlenecks" id="toc-bottlenecks">Bottlenecks</a></li>
</ul></li>
<li><a href="#query-execution-and-distributed-transactions"
id="toc-query-execution-and-distributed-transactions">Query execution
and distributed transactions</a>
<ul>
<li><a href="#parallel-architectures"
id="toc-parallel-architectures">Parallel architectures</a></li>
<li><a href="#distributed-transactions"
id="toc-distributed-transactions">Distributed transactions</a></li>
<li><a href="#replication" id="toc-replication">Replication</a></li>
<li><a href="#eventual-consistency"
id="toc-eventual-consistency">Eventual consistency</a></li>
</ul></li>
<li><a href="#gossip-based-computing"
id="toc-gossip-based-computing">Gossip-based computing</a>
<ul>
<li><a href="#achieving-random-topologies"
id="toc-achieving-random-topologies">Achieving random
topologies</a></li>
<li><a href="#a-generic-gossip-based-substrate"
id="toc-a-generic-gossip-based-substrate">A generic gossip-based
substrate</a></li>
</ul></li>
<li><a href="#dhts" id="toc-dhts">DHTs</a>
<ul>
<li><a href="#distributed-systems-complexity"
id="toc-distributed-systems-complexity">Distributed systems
complexity</a></li>
<li><a href="#architectures"
id="toc-architectures">Architectures</a></li>
<li><a href="#decentralized-architectures"
id="toc-decentralized-architectures">Decentralized
architectures</a></li>
<li><a href="#implementations-of-dhts"
id="toc-implementations-of-dhts">Implementations of DHTs</a></li>
</ul></li>
<li><a href="#consistency-models"
id="toc-consistency-models">Consistency models</a>
<ul>
<li><a href="#when-is-it-needed" id="toc-when-is-it-needed">When is it
needed?</a></li>
<li><a href="#examples-of-consistency-guarantees"
id="toc-examples-of-consistency-guarantees">Examples of consistency
guarantees</a></li>
<li><a href="#strong-consistency" id="toc-strong-consistency">Strong
consistency</a></li>
</ul></li>
</ul>
</nav>
<!-- markdownlint-disable MD010 MD041 MD001 MD036 MD029 MD034-->
<h2 id="storage">Storage</h2>
<h3 id="storage-hierarchy">Storage hierarchy</h3>
<p>Hierarchy (smaller and faster to bigger and slower): CPU registers
&lt; CPU Caches &lt; DRAM &lt; SSD &lt; HDD &lt; Network storage</p>
<p>Out-of-orderness is hard in processors because of the data
dependencies.</p>
<p>Hierarchy is a thing because of the locality - processors want to
reuse the program’s locality in the CPU registers, caches.</p>
<h4 id="non-volatile-memory-vs-solid-state-drive">Non-Volatile Memory vs
Solid-State Drive</h4>
<p>NVM:</p>
<p>Goals - data persists after power-cycle and to reduce
random/sequential access gap and no seek/ rotational delays</p>
<p>like DRAM, low latency loads and stores</p>
<p>like SSD, persistent writes and high density</p>
<p>byte-addressible</p>
<p>SSD:</p>
<p>it uses non-volatile flash chips and SSD controller (embedded
processor, which bridges flash chips to SSD IO interfaces)</p>
<p>block-addressible</p>
<h3 id="file-storage">File storage</h3>
<p>Files are made of pages (the communication DRAM to/from SSD), and in
pages there are fields that are individual informations.</p>
<p>The goal is to get the page such that it would favor locality (would
have more useful material in it).</p>
<h4 id="different-file-organizations">Different file organizations</h4>
<p>Heap files: best when typical access is a full file scan. Hard to
find a file. Simplest implementation is doubly-linked list.</p>
<p>Sorted files: Best for retrieval in an order, or for retrieving a
range. You have to choose a way to sort by.</p>
<p>Log-structured files: it works with an idea of immutability - fast to
insert/delete/update, but for reading file needs to be reconstructed
from logs.</p>
<h3 id="page-layout-for-relational-data">Page layout (for relational
data)</h3>
<h4 id="n-ary-storage-model-nsm">N-ary Storage Model (NSM)</h4>
<p>Taking every row and writing it out in the page. You assume that the
data will be needed in a row manner.</p>
<p>It has a page header, record headers (useful for variable size
records).</p>
<p>When the records are fixed-length, then it is easy to find n-th
record.</p>
<p>When the records are variable-length - it used to be separated by
special chars, but now it uses record headers with pointers.</p>
<p>And now variable-length records use slot array, which also points to
free space, such that the page can be reorganized to use more space
efficiently.</p>
<h4 id="decomposition-storage-model-dsm">Decomposition Storage Model
(DSM)</h4>
<p>Taking columns and put them in the pages.</p>
<p>Initial idea was to decompose all columns and store value of the
column + row ID (for reconstruction)</p>
<p>Pros: saves IO by bringing only relevant attributes (but there are a
lot of infrastucture for that) and very memory compressing columns is
typically easier.</p>
<p>Cons: Writes are more expensive, and need tuple reconstruction.</p>
<p>It is good for compression. Could be Run-length encoding (count the
value and store the count + value), Bit-vector encoding (translate value
into a bit in the bitstring), Partitioning - Dictionary (value -&gt;
number), Frequency partitioning (value -&gt; number only in the page,
where dictionaries are smaller).</p>
<h3 id="partition-attributes-across-pax">Partition Attributes Across
(PAX)</h3>
<p>Decompose a slotted-page internally in mini pages per attribute.</p>
<p>It is cache-friendly, compatible with slotted-pages, retains NSM IO
pattern, and brings only relevant attributes to the cache.</p>
<h2 id="query-execution-and-optimization">Query execution and
optimization</h2>
<h3 id="query-execution">Query execution</h3>
<p>The <strong>processing model</strong> of a DBMS defines how the
system executes a query plan</p>
<h4 id="extreme-1-iterator-model">Extreme 1: iterator model</h4>
<p>Each query operator implements its <strong>next</strong>
function.</p>
<p>On each invocation, the operator returns a single tuple, or empty.
Next recursively calls other operators’ next functions. This way it
passes the tuple through the pipeline and adds it to the query
return.</p>
<p>The DBMS traverses the tree. For each node that it visits, it has to
figure out what the operator needs to do. Same for expressions. This is
done for <strong>every single tuple</strong></p>
<p>Result:</p>
<ul>
<li>Many function calls - save/restore contents of CPU registers, and
force a new instruction stream into the pipeline (which is bad for
instruction cache)</li>
<li>Generic code - has to cover every table, datatype and query</li>
</ul>
<p>It’s like getting one beer at the time and storing it.</p>
<h4 id="extreme-2-block-oriented-model">Extreme 2: block-oriented
model</h4>
<p>Each operator processes its input all at once and emits its output
all at once The operator “materializes” its output as a single result.
Often bottom-up plan processing.</p>
<p>Naive solution for output materialization problem: process a filters
separately for columns and then join them.</p>
<p>Another version: add the filter as extra to the produced filter
result instead of joining them.</p>
<p>It can also use selection vector, which is a bitmap which then is
joined on</p>
<p><strong>Tuple materialization problem</strong> - when joining tables,
the columns can get shuffled, and it cannot use virtual ids and stiching
becomes random access.</p>
<p>Solutions for this:</p>
<ul>
<li>Stich columns before join</li>
<li>Sort lists of table ids before projection</li>
<li>Use order-preserving joins (jive-joins), but this is not always
applicable</li>
</ul>
<p>Pros of block oriented:</p>
<ul>
<li>no next() calls - no per-tuple overhead</li>
<li>typically combined with columnar storage</li>
<li>avoid interpretation when evaluating expressions (most of the
time)</li>
</ul>
<p>Con: ouput materialization is costly in terms of memory bandwidth</p>
<p>It’s like getting beers in full amount, e.g. 100 beers - heavy to
carry</p>
<h4 id="middle-ground-vectorized-iterator-model">Middle ground:
vectorized iterator model</h4>
<p>It’s like getting beers in crates - best of both worlds.</p>
<p>Operator emits vector of tuples instead of a single tuple. The size
of vector must fit in the CPU cache.</p>
<p>It is ideal for OLAP queries - Greatly reduces the number of
invocations per operator. Allows for operators to use vectorized (SIMD)
instructions to process batches of tuples</p>
<h3 id="query-optimization">Query optimization</h3>
<p>For a given query, find the execution plan with the lowest
“cost”.</p>
<p>It is the hardest DBMS component to design/implement correctly. No
optimizer truly produces the “optimal” plan, since it is expensive to
consider all plans (NP-complete), and impossible to get accurate cost of
a plan without executing it!</p>
<p>Optimizers make a huge difference in terms of: Performance,
Scalability, Resource utilization, Database capabilities</p>
<h4 id="multi-dimensional-decision-space">Multi-dimensional decision
space</h4>
<p>In what order to execute operations? (Particularly: relative order of
joins)</p>
<p>Which implementation is best for each operation? (E.g., hash joins,
nested loop joins, sort-merge joins…)</p>
<p>Which access methods to use? (E.g., scan, use of an index)</p>
<p>Suboptimal decisions can have a huge impact! e.g. use of one join
algorithm vs the other, or pushing down selections (that make indexes
useless)</p>
<h4 id="io-of-query-optimizer">IO of query optimizer</h4>
<p>Input - abstract syntax tree created from the query</p>
<p>Output - full physical plan translatable to code.</p>
<h4 id="classic-architecture">Classic architecture</h4>
<p>Cost estimation is used in logical-physical plan loop</p>
<pre class="mermaid"><code>graph TD;
User --SQL query--&gt; parser
parser --AST--&gt; A((Logical plan))
A((Logical plan)) --&gt; B((Physical plan))
B((Physical plan)) --&gt; A((Logical plan))
B((Physical plan)) --&gt; Result</code></pre>
<h4 id="relational-algebra-equivalences">Relational algebra
equivalences</h4>
<p>Key concept in optimization: <strong>Equivalences</strong>. Two
relational algebra <strong>expressions</strong> are said to be
equivalent if on every legal database instance, the two expressions
generate the same set of tuples.</p>
<p>Selections (WHERE clause in SQL) are cascading (s1 and s2 and … of R
=== s1(s2(…(R)))) and commutative (s1(s2(R))===s2(s1(R)))</p>
<p>Projections (SELECT clause in SQL) are cascading on the attributes:
<span class="math inline">\pi_{a_1}(R) \equiv
\pi_{a_1}(...(\pi_{a_n}(R)))</span>, where <span
class="math inline">a_1</span> is a subset of up to <span
class="math inline">a_n</span> projection.</p>
<p>These equivalences allow the push down of selections and projections
before the joins</p>
<p>Joins are commutative and associative. This allows us to choose
different join order.</p>
<h4 id="io-cost-example---naive-example">IO cost example - naive
example</h4>
<p>S: 16000 tuples = 320 pages. T: 256000 tuples = 5120 pages. C: 1600
tuples = 32 pages. Each student takes 16 courses. Each course has 160
students.</p>
<p>Super-Worst scenario / tuple-by-tuple it takes &gt; 500 years:
Cartesian product of fetching a page for each tuple (1 seek per tuple):
#tuples(C) * #tuples(S) * #tuples(T) = 1’600 * 16’000 * 256’000 =
6’553’600’000’000 I/Os. At 2.5ms per I/O -&gt; query takes 519.5
years</p>
<p>Not-Worst-But-Very-Bad scenario (page-by-page) it takes 36 hours:
Cartesian product reading pages at a time, not tuples (1 seek per page)
#pages(C) * #pages(S) * #pages(T) = 32 * 320 * 5120 = 52428800 I/Os
52428800 * 2.5ms = 131072 s -&gt; query takes 36 hours</p>
<h4 id="io-cost-example---educated-approach">IO cost example - educated
approach</h4>
<p>S: 16000 tuples = 320 pages. T: 256000 tuples = 5120 pages. C: 1600
tuples = 32 pages.</p>
<p>Use Block-nested loop joins instead of cross product and push down
projection - 18s</p>
<p>Push down selection and reorder joins - 1s</p>
<h4 id="simple-queries-straightforward-plan">Simple queries,
straightforward plan</h4>
<p>Query planning for OLTP queries is easy because they are
<strong>sargable</strong> (search argument able)</p>
<p>This means just picking the best index, joins are almost always on
foreign key relationships with a small cardinality and can be
implemented with simple heuristics</p>
<h4 id="heuristic-based-optimization">Heuristic-based optimization</h4>
<p>Static rules that transform logical operators into physical plan</p>
<ul>
<li>Perform most restrictive selections early</li>
<li>Perform all selections before joins</li>
<li>Predicate/Limit/Projection pushdowns</li>
<li>Join ordering based on cardinality</li>
</ul>
<p>Example INGRES and Oracle</p>
<p>INGRES has simple relational tables where it’s FK to FK relation.
Therefore it is possible for optimizer to split the query into two,
e.g.:</p>
<p>Goal: Retrieve the names of artists that appear on Joy’s mixtape</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode sql"><code class="sourceCode sql"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">SELECT</span> ARTIST.NAME</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">FROM</span> ARTIST, APPEARS, ALBUM</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">WHERE</span> ARTIST.<span class="kw">ID</span><span class="op">=</span>APPEARS.ARTIST_ID</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>        <span class="kw">AND</span> APPEARS.ALBUM_ID<span class="op">=</span>ALBUM.<span class="kw">ID</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>        <span class="kw">AND</span> ALBUM.NAME<span class="op">=</span><span class="ot">&quot;Joy&#39;s Slag Remix&quot;</span></span></code></pre></div>
<p>Step 1: Decompose into single-variable queries</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode sql"><code class="sourceCode sql"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co">-- Q1</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="kw">SELECT</span> ALBUM.<span class="kw">ID</span> <span class="kw">AS</span> ALBUM_ID <span class="kw">INTO</span> TEMP1</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">FROM</span> ALBUM</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">WHERE</span> ALBUM.NAME<span class="op">=</span><span class="ot">&quot;Joy&#39;s Slag Remix&quot;</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co">-- Q3</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="kw">SELECT</span> APPEARS.ARTIST_ID <span class="kw">INTO</span> TEMP2</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">FROM</span> APPEARS, TEMP1</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">WHERE</span> APPEARS.ALBUM_ID<span class="op">=</span>TEMP1.ALBUM_ID</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co">-- Q4</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="kw">SELECT</span> ARTIST.NAME</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">FROM</span> ARTIST, TEMP2</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">WHERE</span> ARTIST.ARTIST_ID<span class="op">=</span>TEMP2.ARTIST_ID</span></code></pre></div>
<p>Step 2: Substitute the values from Q1→Q3→Q4</p>
<p>Advantages:</p>
<ul>
<li>Easy to implement and debug.</li>
<li>Works reasonably well and is fast for simple queries &amp; small
tables.</li>
</ul>
<p>Disadvantages:</p>
<ul>
<li>Doesn’t <em>truly</em> handle joins.</li>
<li>Join ordering based only on cardinalities.</li>
<li>Naïve, nearly impossible to generate good plans when operators have
complex interdependencies.</li>
<li>Could get stuck at local minima/maxima</li>
</ul>
<h4 id="heuristics-cost-based-optimization">Heuristics + cost-based
optimization</h4>
<p>Use static rules to perform initial optimization. Then use dynamic
programming to determine best join order for tables.</p>
<h5 id="cost-estimation">Cost estimation</h5>
<p>Generate an estimate of the cost of executing a plan for the current
state of the database.</p>
<ul>
<li>Resource utilization (CPU, I/O, network)</li>
<li>Size of intermediate results</li>
<li>Choices of algorithms, access methods</li>
<li>Interactions with other work in DBMS</li>
<li>Data properties (skew, order, placement)</li>
</ul>
<p><strong>Selection without index unsorted</strong>. Cost will change
if. Records are sorted based on the condition attribute. We can utilize
an index to filter out some records. We need to materialize the output
result.</p>
<p><strong>Page-oriented loop join</strong>: For each tuple in the outer
relation R, we scan the entire inner relation S (but use page-loading).
I/O Cost: #pages of R + #pages of R * #pages of S.</p>
<p>How to choose the outer relation to minimize the cost? - Choose order
of R, S, so that #pages of R &lt; #pages of S and Order benefits cost if
tables are of different size</p>
<h5 id="selectivity-estimates">Selectivity estimates</h5>
<p>Estimating intermediary results of query</p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode sql"><code class="sourceCode sql"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">SELECT</span> <span class="op">*</span> <span class="kw">FROM</span> R <span class="kw">WHERE</span> r.age<span class="op">=</span><span class="dv">18</span></span></code></pre></div>
<p>First necessary to estimate cost of operations (e.g. join)</p>
<p>Crude estimation: selectivity = 1/#keys(R.age), estimated #results =
#Records(R)/#keys(R.age). Range queries: length of the range/length of
the domain. Free if there is an index. It is good estimate when values
are uniformly distributed.</p>
<p>Histograms: equi-width and equi-depth. It is higher cost to build and
maintain but higher accuracy.</p>
<h5 id="join-cardinality-estimates">Join cardinality estimates</h5>
<p>Important to reorder joins so that records are filtered as fast as
possible.</p>
<p>Selectivity = 1/max(#keys(R.sid), #keys(S.sid))</p>
<p>Cardinality estimate = #records(R) * #records(S) / max(#keys(R.sid),
#keys(S.sid))</p>
<h4 id="system-r-optimizer">System R Optimizer</h4>
<p>High level idea:</p>
<ul>
<li>Iterate over the possible plans</li>
<li>Estimate the cost of each plan</li>
<li>Return the cheapest to the user</li>
</ul>
<p>Essentially is programs that write other programs.</p>
<h5 id="abstract-steps">Abstract Steps</h5>
<p><strong>Step 1</strong>: break the query up into blocks and generate
the logical opereators for each block - this reduces complexity of each
plan.</p>
<p>Block definition: no nested queries, exactly one SELECT and FROM and
at most one WHERE, GROUPBY, HAVING</p>
<p><strong>Step 2</strong>: for each individual block: for each logical
operator, consider a set of physical operators and offered access paths.
Then iteraticely construct a “left-deep” tree that minimizes the
estimated amount of work to execute the plan. Join order is important
because it gets rid of the most amount of data. Left-join mostly offers
the best performance benefit because the rightmost file can be parsed
and immediately shown. It also offers dynamical programming due to the
fact that BNL can incrementally join together.</p>
<p>Access path - data scan.</p>
<h5 id="system-r-optimizer-steps">System R Optimizer Steps</h5>
<p><strong>Step 1</strong>: Choose the best access path to each
table</p>
<p><strong>Step 2</strong>: Enumerate all possible join orderings for
the tables</p>
<p><strong>Step 3</strong>: Determine the join ordering with the lowest
cost.</p>
<p>At every step only keep the lowest cost or most interesting
order.</p>
<p>If this was Naive - try all possible orders, it would be N! (for N
joins)</p>
<p><strong>Principle of optimality</strong> - the optimal plan for k
joins is produces by extending the optimal plan(s) for k-1 joins. This,
with dynamic programming, ends up being <span class="math inline">O(N
\times 2^^{N-1})</span></p>
<p>This is a very aggressive optimizer, meaning it leaves out some paths
that could be more efficient down the road, but it’s designed to be
faster.</p>
<h4 id="revisited-principle-of-optimality">Revisited principle of
optimality</h4>
<p>Principle of optimality may lead to suboptimal plans - e.g. order is
not considered -&gt; additional cost at the end (avoided by sort merge
join)</p>
<p><strong>Relaxed principle of optimality</strong> - aimed at what the
output is expected to be, e.g. for the ordered output - a plan is
compared with all other plans that produce the same order</p>
<h4 id="revisited-selectivity-estimates">Revisited selectivity
estimates</h4>
<p>if there is no index and no histogram or complex predicates, we
cannot estimate #Keys(R.age).</p>
<p>When everything else fails, revert to magic #Keys(R.age)=10.</p>
<h2 id="execution-models-for-distributed-computing">Execution models for
distributed computing</h2>
<h3 id="big-data">Big data</h3>
<p>The three (plus two) Vs: Big data is high <em>volume</em>, high
<em>velocity</em>, and/or high <em>variety</em> information assets that
require new forms of processing to enable enhanced decision making.</p>
<ul>
<li><em>Volume</em>: The quantity of generated and stored data.</li>
<li><em>Velocity</em>: The speed at which the data is generated and
processed.</li>
<li><em>Variety</em>: The type and nature of the data.</li>
<li><em>Variability</em>: Inconsistency of the data set.</li>
<li><em>Veracity</em>: The quality of captured data.</li>
</ul>
<h3 id="query-models">Query models</h3>
<ul>
<li>Shared-nothing model - Each machine does not share any resources
with any other machine. To communicate to another machine only through
network.</li>
<li>Shared-everything model - Each machine shares everything with other,
therefore all cpus can talk with each other, and access any memory</li>
<li>Shared-memory model - There is a unified memory space for all,
coherency is very easy, but there are problems.</li>
</ul>
<h4 id="shared-nothing-model-message-passing-model">Shared-nothing model
(message-passing model)</h4>
<p>No shared data, therefore we need declustering (spreading data
between disks).</p>
<p>Ways to do it:</p>
<ul>
<li>Attribute-less partitioning (random, round-robin)</li>
<li>Single Attribute Schemes (Hash declustering, Range declustering) -
like sorting</li>
<li>Multiple Attributes schemes possible (MAGIC, BERD etc)</li>
</ul>
<h5 id="hash-declustering">Hash declustering</h5>
<p>Essentially creates a skewed distribution.</p>
<p>Selections with equality predicates referencing the partitioning
attribute are directed to a single node. Therefore less queries are
made</p>
<p>Equality predicates referencing a non-partitioning attribute and
range predicates are directed to all nodes.</p>
<h5 id="range-declustering">Range declustering</h5>
<p>Partition depending on the range.</p>
<p>The more you know about your data or, better, the more you know about
your queries, the better is the distribution.</p>
<p>Equality and range predicates referencing the partitioning attribute
are directed to a subset of nodes.</p>
<p>Predicates referencing a non-partitioning attribute are directed to
all nodes.</p>
<h5 id="declustering-tradeoffs">Declustering tradeoffs</h5>
<p>Range selection predicate using a clustered B+-tree 0.01% selectivity
(result is 10 records). (B+-tree is the tree, where the attributes are
never interfering). The range declustering method is way better than
hash/random/round-robin methods.</p>
<p>If the selectivity goes up to 1% selectivity (result is 1000
records), then ranges drops very quickly, below hash/random/round-robin
methods.</p>
<p>This is because at low selectivity range was distributing the data
all over, so the specific records were found, but then
hash/random/round-robin kept the same spread.</p>
<h4 id="distributed-join">Distributed Join</h4>
<p>partition inputs to buckets, eahc bucket fits in join processors’
aggregate memory</p>
<p>Partition and join each bucket pair across join processors</p>
<h4 id="distributed-aggregation">Distributed Aggregation</h4>
<p>Compute aggregate locally for each node</p>
<p>redistribute by hashing group attribute and aggregate partial
results</p>
<h3 id="mapreduce">MapReduce</h3>
<p>MapReduce approach - code using functional model, hide complexity
behind a library.</p>
<p>It is simple distributed computation on a complex data.</p>
<p>e.g. convert all text to upper case:</p>
<p>Simple mapping:</p>
<ol type="1">
<li>split data file into splits (can be stored in different nodes)</li>
<li>apply map operation to each split</li>
<li>collect all outputs together to get the result</li>
</ol>
<p>MapReduce:</p>
<ol type="1">
<li>split data file into splits (can be stored in different nodes)</li>
<li>apply map operation to each split</li>
<li>use reducers to collect the data in smaller maps</li>
<li>collect all outputs together to get the result</li>
</ol>
<p>MapReduce is simple and scales very well. Problem is the amount of
reads and writes (which is because map and reduces are agnostic to each
other), and the problem is reducer waiting for mapper to finish.</p>
<h3 id="spark">Spark</h3>
<p>Goals:</p>
<ul>
<li>Improve expressiveness and extensibility of model</li>
<li>Make coding easier: strive for high-level code</li>
<li>Enable additional optimizations</li>
<li>Improve performance by better utilizing the hardware</li>
</ul>
<p>It implements very interesting abstractions that help maintain
MapReduce and improves some issues of it.</p>
<h4 id="resilient-distributed-datasets-rdd">Resilient Distributed
Datasets (RDD)</h4>
<p>Collection of elements that is distributed across the network and
it’s single.</p>
<p>It is immutable.</p>
<p>Distributed, fault-tolerant collections of elements that can be
operated in parallel</p>
<p>There is a lineage maintained, because when it is changed the history
can be kept due to its immutability.</p>
<p>Lazily evaluated.</p>
<p>RDDs contain: details about the data, leneage (history) information
to enable recreating a lost split of an RDD (dependencies from other
RDDs and functions/transformations)</p>
<p>Essentially it is dataflow programming.</p>
<h4 id="limitations-of-vanilla-spark">Limitations of vanilla Spark</h4>
<p>RDDs are schema-less, which makes it inefficient (same as accessing
raw text files), and expensive (high space overhead)</p>
<p>Spark has an extension which translates RDDs to data frames.</p>
<h2 id="concurrency-control">Concurrency control</h2>
<h3 id="acid-transaction-schedules">ACID &amp; Transaction
Schedules</h3>
<p><strong>Transaction (txn, or Xact)</strong> - sequence of actions
executed on a shared database to perform some higher-level function.
Basic unit of change in the DBMS.</p>
<p><strong>ACID</strong>:</p>
<ul>
<li><strong>Atomicity</strong> - Either all actions in the txn happen or
none happen</li>
<li><strong>Consistency</strong> - if each txn is consstent and the DB
starts consistent, it ends up consistent</li>
<li><strong>Isolation</strong> - Execution of one txn is isolated from
that of other txns</li>
<li><strong>Durability</strong> - if a txn commits, its effects
persist</li>
</ul>
<p>Txn could either commit after completing all its actions or abort
after executing some of its actions</p>
<p>All transactions are atomic.</p>
<p>Durability relies on logs.</p>
<p>Each transaction must leave the database in a consistent state</p>
<p>Users submit transactions, and expect isolation – each txn executed
by itself. <strong>Concurrency</strong> is very important for the
performance. Net effect identical to executing all txns one after the
other in some serial order.</p>
<p>For concurrency, DBMS uses schedules - a list of actions (reading,
writing, aborting or committing) from a set of txns.</p>
<h4 id="scheduling-transactions">Scheduling transactions</h4>
<ul>
<li><strong>Serial schedule</strong> - schedule that does not interleave
the actions of different transactions.</li>
<li><strong>Equivalent schedules</strong> - for any db state, the effect
(on the set of objects in the db) of executing the first schedule is
identical to the effect of executing the second schedule</li>
<li><strong>Serializable schedule</strong> - a schedule that is
equivalent to some serial execution of the txns.</li>
</ul>
<p>Anomalies with interleaved execution:</p>
<ul>
<li>Dirty reads - WR conflicts - reading uncommitted data</li>
<li>RW conflicts - unrepeatable reads</li>
<li>WW conflicts - overwriting uncommitted data</li>
</ul>
<h4 id="aborting-a-transaction">Aborting a transaction</h4>
<p>If txn is aborted, all its actions need to be undone. And due to
<strong>cascading aborts</strong> the dependent txns need to be aborted
(e.g. if Tj reads an object last written by Ti, Tj must be aborted as
well)</p>
<h4 id="precedence-graph">Precedence graph</h4>
<p>One node per txn, edge from Ti to Tj if Tj reads/writes an object
last read/written by Ti.</p>
<p><strong>Theorem</strong> - a schedule is conflict serializable iff
its dependency graph is acyclic.</p>
<p><strong>Conflict serializable</strong> - if schedule is conflict
equivalent to some serial schedule. I.e. they involve the same actions
of the same txns, every pair of conflicting actions is ordered the same
way -&gt; basically if we can turn the one into the other by swapping
non-conflicting adjacent actions</p>
<h3 id="pessimistic-concurrency-control-protocols">Pessimistic
concurrency control protocols</h3>
<h4 id="lock-based-concurrency-control">Lock-based concurrency
control</h4>
<p>There are 2 ways to prevent incosistencies. Preventing - lock
everything, or detection + correction - let it happen and then fix
everything if anyting goes wrong.</p>
<p>Locking protocol guarantees that schedule will be conflict
serializatble (correct) if it completes. And the question is when to
hold the lock.</p>
<p>Locking granularity can be anything: tables, indexes, pages,
records.</p>
<h5 id="two-pahse-locking-2pl-protocol">Two-pahse locking (2PL)
Protocol</h5>
<p>Rule 1: Shared and exclusive locking (corresponds to read/write
locks)</p>
<p>Rule 2: a transaction (txn) cannot request additional locks once it
releases any locks.</p>
<p>2PL allows only schedules who precedence graph is acyclic, therefore
it’s serializable</p>
<p>Strict 2PL only allows locking (meaning no unlocking), and unlocks
only when transaction is committed.</p>
<p>Deadlock - T1 is waiting for a lock which is held by T2, T2 is
waiting for T3, and T3 is waiting for T1. To get out of this everything
needs to be killed. Deadlock detection is very expensive, but there is
deadlock prevention algorithm</p>
<h3 id="optimistic-concurrency-control-protocols">Optimistic concurrency
control protocols</h3>
<p>No locking because conflicts are rare.</p>
<h4 id="kung-robinson-model">Kung-Robinson Model</h4>
<p><strong>Idea</strong>: Every txn is ordered by the exact time it
arrived to the system. While txn exectes, it collects its write set and
read set. After which there is a validation phase. Validation phase
checks that all conflicting actions occurred in the same order. Either
it gets validated and writes are commited to the storage, or invalidated
and not written.</p>
<p>This relies on the timestamps of the txns.</p>
<p>There are 4 cases for a txn that arrives for it to check previous
txns, For all i and j such that Ti &lt; Tj, check that Ti completes
before Tj begins.:</p>
<ul>
<li>that Ti already finished</li>
<li>that Ti writes before Tj writes
<ul>
<li>does Tj read dirty data? -&gt; to check, the Tj read set does not
intersect with write set of Ti</li>
</ul></li>
<li>that Ti reads before Tj reads
<ul>
<li>does Ti overwrite Tj’s writes? -&gt; to check, the Tj write set does
not intersect with write set of Ti (Tj’s one should persist)</li>
</ul></li>
</ul>
<h4 id="comments-on-validation">Comments on validation</h4>
<p>Validation is a critical section, and nothing else goes on
concurrently. BUT if the validation/write phase is long, then it is
major drawback.</p>
<p>Optimization for read-only txns: shorter critical section because
there is no Write phase.</p>
<h3 id="timestamp-based-cc">Timestamp-based CC</h3>
<p><strong>Continuous validation</strong> - not a distinct phase</p>
<p>Read and write timestamps per object, which means the validation
happens after each action. If the validation fails, we abort the
transaction</p>
<p>There are 4 actions to choose after the comparison of txn timestamp
with read/write timestamps of the objects: continue, abort, commit, skip
write</p>
<blockquote>
<p>When the validation fails, the new txn is created with a new
timestamp. Then validation is running again with newly completed
txns.</p>
</blockquote>
<p><strong>Idea:</strong> txn timestamp TS begin time</p>
<p><strong>Object</strong>: read-timestamp (RTS) and a write-timestamp
(WTS)</p>
<p>When txt T wants to <strong>READ</strong> object O:</p>
<ul>
<li>TS(T) &lt; WTS(O): violates timestamp order of T w.r.t. writer of O.
<ul>
<li>Abort T and restart it with a new, higher TS.</li>
</ul></li>
<li>TS(T) &gt;= WTS(O):
<ul>
<li>Allow T to read O.</li>
<li>Reset RTS(O) to max(RTS(O), TS(T))</li>
</ul></li>
<li>Change to RTS(O) on reads must be written in some persistent fashion
🡪 overhead.</li>
</ul>
<p>When txt T wants to <strong>WRITE</strong> object O:</p>
<ul>
<li>TS(T) &lt; RTS(O): violates timestamp order of T w.r.t. reader of O
🡪 abort and restart T.</li>
<li>TS(T) &lt; WTS(O) 🡪 violates timestamp order of T w.r.t. writer of
O. 🡪 ???
<ul>
<li>Thomas Write Rule: Outdated write 🡪 Safely ignore the write – it’s
as if the write happened before and was overwritten</li>
<li>need not restart T!</li>
<li>Allows some serializable schedules (correct) that are not conflict
serializable.</li>
</ul></li>
<li>Else, allow T to write O (and update WTS(O)).</li>
</ul>
<h3 id="multiversion-cc">Multiversion CC</h3>
<p>Recognising the fact that most transactions read all the time.</p>
<p>Goal: txn never waits on read</p>
<p><strong>Idea</strong>: Maintain several versions of each database
object (multi-version), each with a read and a write timestamp.
Transaction Ti reads the most recent version whose write timestamp
precedes TS(Ti).</p>
<h4 id="writer-txn">Writer txn</h4>
<p>To read an object, follow reader protocol</p>
<p>To write an object:</p>
<ul>
<li>finds newest version V</li>
<li>RTS(V) &gt; TS(T) - reject write</li>
<li>RTS(V) &lt;= TS(T) - T makes a copy CV of V, with a pointer to V,
with WTS(CV) = TS(T), RTS(CV) = TS(T) (write is buffered/locked until T
commits, other txns cannot read version CV, such that every txn’s effect
need to persist for the txns that follow)</li>
</ul>
<h3 id="bottlenecks">Bottlenecks</h3>
<p>lock thrashing - 2PL, strict 2PL</p>
<p>timestamp allocation - all T/O algorithms + deadlock prevention</p>
<p>memory allocation - MVCC, OCC</p>
<h4 id="improving-performance">Improving performance</h4>
<p><strong>Snapshot isolation</strong> - take the whole database
snapshot, and if no conflicting writes were made, take the whole
snapshot.</p>
<p>Snapshot isolation (SI) is the most popular isolation guarantee in
real DBMS.</p>
<ul>
<li>all txn reads will see a consistent snapshot of the database</li>
<li>the txn successfully commits only if no updates it has made conflict
with any concurrent updates made since that snapshot.</li>
</ul>
<p>SI does not guarantee serializability!</p>
<h2 id="query-execution-and-distributed-transactions">Query execution
and distributed transactions</h2>
<h3 id="parallel-architectures">Parallel architectures</h3>
<h4 id="shared-memory">Shared memory</h4>
<p>Nodes share both RAM and disk. Dozens to hundreds of processors, easy
to use and program, BUT expensive to scale.</p>
<h4 id="shared-disk">Shared disk</h4>
<p>All nodes access same disks (which are found in the largest
non-cluster multiprocessors)</p>
<p>Traditionally hard to scale past a certain point - due to contention
on storage bandwidth. Existing deployments have &lt; 10 machines</p>
<p>However due to cloud incarnation - running over Amazon s3 or other
service, there is an arbitrary scaleout because <strong>S3 has a lot of
disks and executors are stateless</strong>.</p>
<h4 id="shared-nothing">Shared nothing</h4>
<p>Cluster machines on high-speed network. Where each machine has its
own memory and disk. (each machine might run several nodes in it).</p>
<p>Characteristics: most scalable today (because of lowest contention)
but hard to manage and tune (e.g. because of data rebalancing when
adding a new node)</p>
<h3 id="distributed-transactions">Distributed transactions</h3>
<h4 id="single-node-vs-distributed-txns">Single node vs distributed
txns</h4>
<p>Single-node txn accesses the data on one partition, whereas distr txn
accesses data at one or more partitions, so it requires expensive
coordination between concurrent transactions.</p>
<h4 id="transaction-coordination">Transaction coordination</h4>
<p>Two types:</p>
<ul>
<li><strong>Centralized</strong> - global “traffic coordinator”</li>
<li><strong>Decentralized</strong> - nodes organize themselves.</li>
</ul>
<h4 id="distributed-concurrency-control">Distributed concurrency
control</h4>
<p>Need to allow multiple transactions to execute simultaneously across
multiple nodes.</p>
<p>Many of the protocols from single-node DBMS can be adapted</p>
<p>It is harder because of:</p>
<ul>
<li>network communication overhead</li>
<li>clock skew</li>
<li>node failures</li>
<li>replication</li>
<li>distributed 2PL:
<ul>
<li>increased clock duration</li>
<li>who detects deadlocks?</li>
</ul></li>
<li>timestamp-based:
<ul>
<li>whose clock is correct?</li>
<li>can we have 1 global clock?</li>
</ul></li>
</ul>
<h5 id="distributed-locking">Distributed locking</h5>
<ul>
<li>Centralized: One site does all locking.
<ul>
<li>Vulnerable to single site failure.</li>
</ul></li>
<li>Primary Copy: All locking for an object done at the primary copy
site for this object.
<ul>
<li>Reading requires access to locking site as well as site where the
object is stored.</li>
</ul></li>
<li>Fully Distributed: Locking for a copy done at site where the copy is
stored.
<ul>
<li>Locks at all sites while writing an object</li>
</ul></li>
</ul>
<h5 id="distributed-recovery">Distributed recovery</h5>
<p>If sub-transactions of an txn execute at different sites, all or none
must commit</p>
<p>A log is maintained at each site, as in a centralized DBMS, and
commit protocol actions are additionally logged.</p>
<h4 id="commits">Commits</h4>
<p>When a multi-node transaction finishes, the DBMS needs to ask all of
the nodes involved whether it is safe to commit.</p>
<p>Nodes must use an <strong>atomic commit protocol</strong> - all nodes
have to agree. E.g.:</p>
<ul>
<li>Two-phase commit</li>
<li>Three-phase commit (many assumptions; not used)</li>
<li>Paxos</li>
<li>Raft</li>
<li>ZAB (Apache Zookeeper)</li>
</ul>
<h4 id="pc-two-phase-commit">2PC (Two-phase commit)</h4>
<p>Site at which Xact originates is coordinator; other sites at which it
executes are subordinates.</p>
<p>First phase - collect information.</p>
<p>Second phase - implement a decision.</p>
<p>When txn wants to commit:</p>
<ul>
<li>coordinator sends <strong>prepare</strong> msg to each subordinate
(do you all agree that this txn should be committed?)</li>
<li>subordinate force-writes an <strong>abort</strong> or
<strong>prepare</strong> log records and then sends a <em>no</em> or
<em>yes</em> msg to coordinator.</li>
<li>if coordinator gets unanimous yes votes, force-writes a
<strong>commit</strong> log records and sends a <strong>commit</strong>
msg to all subs. Else, force-writes abort log rec, and sends
<strong>abort</strong> msg</li>
<li>Subordinates force-write <strong>abort/commit</strong> log rec based
on msg they get, then send <strong>ack</strong> msg to coordinator.</li>
<li>Coordinator writes <strong>end</strong> log rec after getting all
acks.</li>
</ul>
<blockquote>
<p>Recovery system:</p>
<ol type="1">
<li>log records (ARIES style) - long record, and any time there is a new
transaction it adds its actions to the log. One reason is if the txn was
aborted, it needs to be <strong>undone</strong>. Another reason -
failure to make the value to the disk - action needs to be
<strong>redone</strong>.</li>
<li>Redo then undo - txn table where there are winners and losers, so
winners need to redo actions, and losers undo actions</li>
</ol>
</blockquote>
<p>Commets on 2PC:</p>
<ul>
<li>a lot of messages because of the two rounds of communication</li>
<li>any site can decite to abort a txn, all sites must agree to
commit.</li>
<li>Every msg reflects a decision by the sender; <strong>to ensure that
this decision survives failures, it is first recorded in the local
log</strong></li>
<li>Because of logging 2PC is expensive because of the force-writes - we
need to wait for the log to be on disk before moving on.</li>
</ul>
<p>ACKs are sent to ensure everyone knows final outcome.</p>
<p>Subordinates force-write the log records so that they do not need to
ask the coordinator about the info.</p>
<h4 id="pc-with-presume-commit-optimization-on-2pc">2PC with presume
commit (optimization on 2PC)</h4>
<p>Because of the idea that most txns usually commit, I assume that
everything is fine.</p>
<p>Cheaper to Require ACKs for Aborts and to Eliminate ACKs for
commits.</p>
<p>Force only <strong>abort*</strong> (star means forced), no
information means commit! Problem - Commit after crash of the
coordinator after sending out “prepare”!</p>
<p>Record subordinate names before prepared state ⇒Subordinates as in
PA; coordinator writes collecting* (star means forced) ⇒Read-only
optimizations apply here</p>
<h4 id="pc-with-presume-abort-optimization-on-2pc">2PC with presume
abort (optimization on 2PC)</h4>
<p>This is the opposite to presumed commit, but it’s not used.</p>
<h4 id="pc-coordinator-failures-blocking">2PC Coordinator failures &amp;
Blocking</h4>
<p>If coordinator for Xact T fails, subordinates who have voted yes
cannot decide whether to commit or abort T until coordinator recovers.
<strong>T is blocked</strong> and even if all subordinates know each
other (extra overhead in prepare msg) they are blocked unless one of
them voted no.</p>
<h3 id="replication">Replication</h3>
<p>Copying data is annoying but necessary for availability, and
failures.</p>
<p>Replication:</p>
<ul>
<li>keep several copies of the data in other servers</li>
<li>if a server fails, another server takes over</li>
<li>use for load balancing</li>
</ul>
<p><strong>Synchronouse replication</strong>: All copies of a modified
relation (fragment) must be updated before the modifying Xact
commits.</p>
<p><strong>Asynchronous Replication</strong>: Copies of a modified
relation are only periodically updated; different copies may get out of
sync in the meantime.</p>
<h4 id="synchronous-replication">Synchronous replication</h4>
<p>Two versions:</p>
<ul>
<li>Voting: Xact <strong>must write a majority</strong> of copies to
modify an object; <strong>must read enough copies</strong> to be sure of
seeing at least one most recent copy</li>
<li>Read-any Write-all: Writes are slower and reads are faster, relative
to Voting. (Most common approach to synchronous replication)</li>
</ul>
<p>Cost - very expensive:</p>
<ul>
<li>Before an update Xact can commit, it must obtain locks on all
modified copies:
<ul>
<li>Sends lock requests to remote sites, and while waiting for the
response, holds on toother locks!</li>
<li>If sites or links fail, Xact cannot commit until they are back
up.</li>
<li>Even if there is no failure, committing must follow an expensive
commit protocol with many msgs.</li>
</ul></li>
</ul>
<p>Due to the cost, alternative of asynchronous replication is widely
used in NoSQL systems (eventual consistency)</p>
<h4 id="asynchronous-replication">Asynchronous replication</h4>
<p>Allows modifying Xact to commit before all copies have been changed
(and readers nonetheless look at just one copy).</p>
<p>Two approaches: <strong>Primary Site</strong> and
<strong>Peer-to-Peer</strong> replication. - Difference lies in how many
copies are <em>“updatable”</em> or <em>“master copies”</em></p>
<h5 id="primary-site">Primary site</h5>
<p>Exactly one copy of a relation is designated the primary or master
copy. Replicas at other sites cannot be directly updated.</p>
<ul>
<li>The primary copy is published.</li>
<li>Other sites subscribe to (fragments of) this relation; these are
secondary copies.</li>
</ul>
<p>Main issue: How are changes to the primary copy propagated to the
secondary copies?</p>
<p>– Done in two steps. First, capture changes made by committed Xacts;
then apply these changes.</p>
<p><strong>Implementing the Capture step</strong>:</p>
<ul>
<li><strong>(Physical) Log-Based Capture</strong>: The log (kept for
recovery) is used to generate a Change Data Table (CDT). This is faster
one.
<ul>
<li>If this is done when the log tail is written to disk, must somehow
remove changes due to subsequently aborted Xacts.</li>
</ul></li>
<li><strong>Procedural Capture</strong>: A procedure that is
automatically invoked (ex: trigger) does the capture; typically, just
takes a snapshot. This is slower but more portable.</li>
<li>Log-Based Capture is cheaper &amp; faster, but relies on proprietary
log details.</li>
<li>Middle-ground: Logical log-based capture
<ul>
<li>[MySQL] Row-based replication: Describe edits at row
granularity</li>
</ul></li>
</ul>
<p><strong>Implementing the Apply step</strong>:</p>
<p>The Apply process at the secondary site periodically obtains (a
snapshot or) changes to the CDT table from the primary site, and updates
the copy. Replica can be a view over the modified relation!</p>
<p>Log-Based Capture plus continuous Apply minimizes delay in
propagating changes.</p>
<p>Procedural Capture plus application-driven Apply is the most flexible
way to process changes.</p>
<h4 id="peer-to-peer-multi-leader-replication">Peer-to-Peer
(multi-leader) Replication</h4>
<p>More than one of the copies of an object can be a master in this
approach.</p>
<p>Changes to a master copy must be propagated to other copies
somehow.</p>
<p>If two master copies are changed in a conflicting manner, this must
be resolved. (e.g., Site 1: Joe’s age changed to 35; Site 2: to 36)</p>
<h3 id="eventual-consistency">Eventual consistency</h3>
<p>Tracking mutable, replicated state. Due to replication we need to
decide: Synchronous or asynchronous? And Can processes read from / write
to any replica? What are we going to do in case of the failure?</p>
<h4 id="cap-theorem">CAP Theorem</h4>
<p><strong>Consistency</strong> - linearizability</p>
<p><strong>Availability</strong> - all alive nodes can satisfy all
requests</p>
<p><strong>Partition Tolerance</strong> - operate correctly despite
message loss</p>
<p>We cannot have all 3 at once.</p>
<h2 id="gossip-based-computing">Gossip-based computing</h2>
<h3 id="achieving-random-topologies">Achieving random topologies</h3>
<h4 id="peer-sampling-service">Peer sampling service</h4>
<p>Provide sample of the network to each node.</p>
<p>How: by building a random graph with an out-degree of f. In a
decentralized way.</p>
<p>Goal: create an overlay network. Provide each peer with a random
sample of the network.</p>
<p>Means: gossip-based protocol - what data should be gossiped? To whom?
How to process the exchanged data?</p>
<p>Result is “who knows who” graphs - overlay of Properties (degree,
clustering, diameter, etc.), Resilience to network dynamics, Closeness
to random graphs</p>
<p>Objective is to provide nodes with a peer drawn uniformly at random
from the complete set of nodes. Sampling is accurate: reflects the
current set of nodes. Independent views. Scalable service</p>
<p>Using gossip, we will periodically exchange the knowledge of the
system.</p>
<h5 id="system-model-of-the-service">System model of the service</h5>
<ul>
<li>System of n peers</li>
<li>Peers join and leave (and fail) the system dynamically and are
identified uniquely (IP @)</li>
<li>Epidemic interaction model:
<ul>
<li>Peers exchange some membership information periodically to update
their own membership information</li>
<li>Reflect the dynamics of the system</li>
<li>Ensures connectivity</li>
</ul></li>
<li>Each peer maintains a local view (membership table) of c entries
<ul>
<li>Network @ (IP@)</li>
<li>Age (freshness of the descriptor)</li>
<li>Each entry is unique</li>
<li>Ordered list</li>
</ul></li>
<li>Active and passive threads on each node</li>
</ul>
<h5 id="operations-on-partial-views-membership">Operations on partial
views (membership)</h5>
<ul>
<li>selectPeer() - returns an item</li>
<li>permute() - randomly shuffles items</li>
<li>increaseAge() - forall items add 1 to age</li>
<li>append(…) - append a number of items</li>
<li>removeDuplicates() - remove duplicates (on same address), keep
youngest</li>
<li>removeOldItems(n) - remove n descriptors with highest age</li>
<li>removeHead(n) - remove n first descriptors</li>
<li>removeRandom(n) - remove n random descriptors</li>
</ul>
<h5 id="threads-of-the-peer">Threads of the peer</h5>
<blockquote>
<p>assume no byzantine behaviour</p>
</blockquote>
<p>Active thread:</p>
<pre class="pseudocode"><code>Wait (T time units) // T is the cycle length

p &lt;- selectPeer() // Sample a live peer from the current view

if push then // Takes initiative
  myDescriptor &lt;- (my@,0)
  buffer &lt;- merge (view, {myDescriptor}) //temporary list
  view.permute() //shuffle the items in the view
  move oldest h items to end of the view //to get rid of old nodes
  buffer.append(view.head(c/2)) // copy first half of the items
  send buffer to p
else send{} to p //triggers response

if pull then
  receive buffer from p
  view.selectView(c,h,s,buffer)

view.increaseage(viewp)</code></pre>
<p>Passive thread</p>
<pre class="pseudocode"><code>Do forever

Receive bufferp from p

if pull then
  myDescriptor &lt;-(my@,0)
  buffer &lt;-merge(view,{myDescriptor}) // here we append ourselves with age 0
  view.permute ()
  move oldest h items to end of the view
  buffer.append(view.head(c/2))
  send buffer to p

view.selectView(c,h,s,buffer)
view.increaseage(view_p)</code></pre>
<h5 id="design-space">Design space</h5>
<p>Periodically each peer initiates communication with another peer</p>
<p><strong>Peer selection</strong>:</p>
<p>selectPeer(): returns a live peer from the current view</p>
<ul>
<li>Rand: pick a peer uniformly at random</li>
<li>Head: pick the “youngest” peer</li>
<li>Tail: pick the “oldest” peer</li>
</ul>
<p>Note that <em>head</em> leads to correlated views -
self-reinforcement leads to bias of the view.</p>
<p><strong>Data exchange</strong> (View propagation) - How peers
exchange their membership information?</p>
<ul>
<li>push: Node sends descriptors to selected peer</li>
<li>pull: Node only pulls in descriptors from selected peer</li>
<li>pushpull: Node and selected peer exchange descriptors</li>
</ul>
<p>Pulling alone is pretty bad: a node has no opportunity to insert
information on itself. Potential loss of all incoming connections.</p>
<p>Buffer (h)</p>
<ul>
<li>initialized with the descriptor of the gossiper</li>
<li>contains c/2 elements</li>
<li>ignore h “oldest”</li>
</ul>
<p>Communication model</p>
<ul>
<li>Push: buffer sent</li>
<li>Push/Pull: buffers sent both ways</li>
<li>(Pull: left out, the gossiper cannot inject information about
itself, harms connectivity)</li>
</ul>
<p><strong>Data processing</strong> (View selection): Select (c, buffer)
where c: size of the resulting view, and Buffer: information
exchanged</p>
<p>Select(c,h,s,buffer):</p>
<ol type="1">
<li>Buffer appended to view</li>
<li>Keep the freshest entry for each node</li>
<li>h oldest items removed</li>
<li>s first items removed (the one sent over)</li>
<li>Random nodes removed</li>
</ol>
<p>Merge strategies</p>
<ul>
<li>Blind (h=0,s=0): select a random subset</li>
<li>Healer (h=c/2): select the “freshest” entries</li>
<li>Shuffler (h=0, s=c/2): minimize loss</li>
</ul>
<p>Where:</p>
<ul>
<li>c: size of the resulting view</li>
<li>h: self-healing parameter</li>
<li>s: shuffle</li>
<li>Buffer: information exchanged</li>
</ul>
<h5 id="existing-systems">Existing systems</h5>
<p>Lpbcast [Eugster &amp; al, DSN 2001,ACM TOCS 2003]</p>
<ul>
<li>Node selection: random</li>
<li>Data exchange: push</li>
<li>Data processing: random</li>
</ul>
<p>Newscast [Jelasity &amp; van Steen, 2002]</p>
<ul>
<li>Node selection: head</li>
<li>Data exchange : pushpull</li>
<li>Data processing : head</li>
</ul>
<p>Cyclon [Voulgaris &amp; al JNSM 2005]</p>
<ul>
<li>Node selection: random</li>
<li>Data exchange : pushpull</li>
<li>Data processing : shuffle</li>
</ul>
<p>Comparison metrics:</p>
<ul>
<li>Degree distribution</li>
<li>Average path length</li>
<li>Clustering coefficient</li>
</ul>
<h3 id="a-generic-gossip-based-substrate">A generic gossip-based
substrate</h3>
<p>Each node maintains a set of neighbors (c entries)</p>
<p>Periodic peerwise exchange of information</p>
<p>Each process runs an active and passive threads</p>
<p>And the parameter space for this is peer selection, data exchange and
data processing</p>
<p>This is useful for many things, nowadays for decentralized learning,
it used to be for graph parting.</p>
<h4 id="gossip-based-aggregation">Gossip-based aggregation</h4>
<p>Each node holds a numeric value s</p>
<p>Aggregation function: average over the set of nodes</p>
<p>Active thread:</p>
<pre class="pseudocode"><code>do exactly once in each consecutive delta time units at randomly picked neighbor:

  q = GetNeighbour()
  send s_p to q

  s_q = receive(q)
  s_p = Update(s_p, s_q)</code></pre>
<p>Passive thread:</p>
<pre class="pseudocode"><code>do forever:
  s_q = receive(*)
  send s_p to sender(s_q)
  s_p = Update(s_p, s_q)</code></pre>
<ul>
<li>Assume getneighbor() returns a uniform random sample</li>
<li>Update(sp,sq) returns (sp + sq)/2</li>
<li>Operation does not change the global average but redistributes the
variance over the set of all estimates in the system</li>
<li>Proven that the variance tends to zero</li>
<li>Exponential convergence</li>
</ul>
<h5 id="counting-with-gossip">Counting with gossip</h5>
<ol type="1">
<li>Initialize all nodes with value 0 but the initiator</li>
<li>Global average = 1/N</li>
<li>Size of the network can be easily deduced</li>
<li>Robust implementation
<ol type="1">
<li>Multiple nodes start with their identifier</li>
<li>Each concurrent instance led by a node</li>
<li>Message and data of an instance tagged with a unique Id</li>
</ol></li>
</ol>
<h5 id="ordered-slicing">Ordered slicing</h5>
<p>Each node has a value (storage). We want to separate them in separate
same-size groups. E.g. split the memory load to 3 groups: first 33%,
second 33%, last 33%</p>
<ul>
<li>Create and maintain a partitioning of the network</li>
<li>Each node belongs to one slice</li>
<li>Ex: 20% of nodes with the largest bandwidth</li>
<li>Network of size N</li>
<li>Each node <em>i</em> has an attribute xi</li>
<li>We assume that values (x1 , xN) can be ordered</li>
<li>Problem: automatically assign a slice (top 20%) for each node</li>
</ul>
<p>Each time there is a problem with matching random value with the
actual node value (sorted), we exchage the random numbers between those
values. Then the resulting (initially random) values will be
representing the slices correctly.</p>
<h2 id="dhts">DHTs</h2>
<h3 id="distributed-systems-complexity">Distributed systems
complexity</h3>
<p>No global clock, so no single global notion of the correct titme</p>
<p>Unpredictable failures of components</p>
<p>Highly variable bandwidth</p>
<p>possibly large and variable latency - few ms to several seconds</p>
<h3 id="architectures">Architectures</h3>
<p>client server, datacenters/cloud, decentralized</p>
<h3 id="decentralized-architectures">Decentralized architectures</h3>
<h4 id="p2p-applications">P2P applications</h4>
<p>~50% of the Internet traffic</p>
<p>Applications - mastadon, torrent, blockchain, streaming protocols,
decentralized AI</p>
<h4 id="why-is-it-interesting">Why is it interesting</h4>
<ul>
<li>End-nodes are promoted to active components</li>
<li>Nodes participate, interact, contribute to the services they
use.</li>
<li>Harness huge pools of resources accumulated in millions of
end-nodes.</li>
<li>Avoid a central/master entity</li>
<li>Irregularities and dynamicity are treated as the norm</li>
</ul>
<h4 id="overlay-networks">Overlay networks</h4>
<p>Decentralized system implemented over decentralized network. This is
higher layer over the internet</p>
<p>Unstructed overlays - random graphs that are hard to deal with
because it requires flooding for search.</p>
<p>Structured overlays - nodes, in addition to identification by IP
address, also have a role of being in a position in that structure.</p>
<h4 id="hash-tables">Hash tables</h4>
<p>Data structure that has put() and get() operations, and has efficient
indexing such that read is quick.</p>
<p>Distributed hash table does the same thing but across millions of
hosts on the internet. P2P infrastructure ensures mapping between keys
and physical nodes. No node has entire view of the system but has enough
local knowledge to route the key to the node.</p>
<h4 id="distributed-hash-table">Distributed hash table</h4>
<p>partitions data in large-scale distributed system:</p>
<ul>
<li>Tuples in a global database engine</li>
<li>Data blocks in a global file system</li>
<li>Files in a P2P file-sharing system</li>
</ul>
<p>Lookup would be in a structured overlay,</p>
<h3 id="implementations-of-dhts">Implementations of DHTs</h3>
<h4 id="pastry">Pastry</h4>
<p><strong>P2P routing infrastructure</strong></p>
<p>Overlay: network abstraction on top of IP</p>
<p>Basic functionality: distributed hash table:
<code>key = SHA-1(data)</code></p>
<p>An identifier is associated to each node
<code>nodeId = SHA-1(IP address)</code></p>
<p>Large identifier space (keys and nodeId)</p>
<p>A node is reposible for a range of keys</p>
<p>Routing: search efficiently for keys</p>
<p><strong>Object distribution</strong></p>
<p>128 bit cirular id space (0 to <span class="math inline">2^^{128} -
1</span>) - <em>nodeIds</em> (uniform random) and <em>objIds</em>
(uniform random)</p>
<p>node with numerically closest <em>nodeId</em> maintains that
object.</p>
<p>There can be unbalance to the system, e.g. when a file is accessed
frequently, so the responsible node will have higher traffic</p>
<p><strong>Naming space</strong> - Ring of 128 bit integers, and
<em>nodeIds</em> chosen at random</p>
<p><strong>Key/node mapping</strong> - key associated to the node with
the numerically closest node id</p>
<p><strong>Routing table</strong> - to have efficient routing such that
if churn is experienced, it is still possible to access the data.</p>
<p><strong>Leaf set</strong> - 8 or 16 closest numerical neighbors in
the naming space</p>
<h5 id="pastry-routing-table">Pastry routing table</h5>
<p>We split the circle such that in each part there is a path from node
to a representative from a different part.</p>
<p>Routing tables based on prefix matching:</p>
<ul>
<li>Identifiers are a set of digits in base 16</li>
<li>Matrix of 128/4 lines et 16 columns</li>
<li>routeTable(i,j):
<ul>
<li>nodeId matching the current node identifier up to level I</li>
<li>with the next digit is j</li>
</ul></li>
</ul>
<p><strong>Example:</strong></p>
<p>Consider a peer with id 01110100101 Maintains a neighor peer in each
of the following prefixes: 1, 00, 010, 0110, …</p>
<p>At each routing step, forward to a neigbor with the largest matching
prefix</p>
<h5 id="pastry-routing">Pastry routing</h5>
<p>Leaf set helps with partitioning, i.e. if there are too many nodes
that are disconnected.</p>
<p>Each node will contain 8 or 16 all the closest neighbors, which also
enables direct access to the node when the prefix matches.</p>
<p>Routing algo is week 8 slide</p>
<h5 id="node-departure">Node departure</h5>
<p>Explicit departure or failure</p>
<p>Graceful replacement of a node</p>
<p>The leafset of the closest node in the leafset contains the closest
new node, not yet in the leafset.</p>
<p>Update from the leafset information - since we’re already routing in
the correct partition, and the main replica node fails, the neighboring
node will have the replica and it will be found through leafset.</p>
<p>Update the application.</p>
<h5 id="state-maintenance">State maintenance</h5>
<p><strong>Leaf set</strong> (maintained with pings), is aggressively
monitored and fixed; and is eventual guarantee up to L/2 nodes with
adjacent nodeIds fail simultaineously.</p>
<p><strong>Routing table</strong> is lazily repaired - when a hole is
detected during the routing. It is periodic gossip-based maintenance</p>
<h5 id="reducing-latency">Reducing latency</h5>
<p><strong>Random assignment of nodeId</strong> - nodes numerically
close are geographically (topologically) distant</p>
<p>Objective - fill the routing table with nodes so that routing hops
are as short (latency wise) as possible</p>
<p>Topological metric is <em>latency</em></p>
<h2 id="consistency-models">Consistency models</h2>
<p>Replication is key to availability (low latency, failure resilience,
load balancing). But it creates inconsistencies due to concurrent
accesses.</p>
<h3 id="when-is-it-needed">When is it needed?</h3>
<p>Whenever objects are replicated</p>
<p>Replicase must be consistent in some way: modifications have to be
carried out on all copies, and in the presence of concurrent
updates/reads</p>
<p>Differenc consistency models - a consistency model is a set of rueles
that process obeys while accessing data. Edge cases - strong consistency
(having the latest updates), and eventual consistency (maintain the
replica, such that eventually everyone will have the same data)</p>
<h3 id="examples-of-consistency-guarantees">Examples of consistency
guarantees</h3>
<p>Strong consistency - seee all previous writes</p>
<p>Eventual consistency - see subset of previous writes</p>
<p>Consistent prefix - see initial sequence of writes</p>
<p>Monotonic freshness - see increasing sequence of writes</p>
<p>Read my writes - see all writes performed by reader</p>
<p>Bounded staleness - see all “old” writes</p>
<h3 id="strong-consistency">Strong consistency</h3>
<p>Aka linearizability , one-copy serializability</p>
<p>The responses to the operations invoked in an execution are the same
as if all operations were executed in a sequential order and this order
respects those specified by each process</p>
<p>Strong consistency is impossible to achieve in the presence of
partition (CAP-next lecture)</p>
<p>Strong consistency is impossible to achieve in an asynchronous system
without assumptions on message delivery latencies (FLP) - thinking the
node is slow but it actually failed or vice-versa. To fix this we have a
timeout.</p>
<p><strong>Guarantee</strong>: see all previous writes. All reads at
time t should reflect all the writes that happened before t.</p>
</body>
</html>
